Textbook,Display Book Name,Chapter Number,Chapter Title,Category,Scenario Instances,Target Audience,Chapter Summary,URL,Code snippet,Code snippet description,Libraries
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),4,Easier Python Packaging with flit,Scenario,Others/Generic,Generic,"The chapter presents flit, a Python library that simplifies the process of submitting a Python package to the Python Package Index (PyPI). The traditional process of submitting a package requires creating a setup.py file which can be tedious, whereas flit can create a configuration file interactively. flit requires only minimal package information, and for straightforward packages, it is simple to upload them to PyPI. The chapter provides step-by-step instructions on how to install and use flit, as well as how to complete the additional steps necessary to submit a package to PyPI. The simplified process that flit provides helps developers to more easily package and distribute their Python projects.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"from setuptools import setup

setup(
    name=""my_package"",
    version=""0.1"",
    author=""John Doe"",
    author_email=""johndoe@example.com"",
    description=""A description of my package"",
    packages=[""my_package""],
    install_requires=[""numpy>=1.13.3"", ""matplotlib>=2.1.0""],
)","In the snippet, a typical `setup()` function is used to define metadata about the package and its dependencies. The example shows how to define the `name`, `version`, `description`, `packages`, and `install_requires` for the package and specifies that it requires numpy and matplotlib libraries to be present in the specified version or later. This `setup()` function can be run on its own or with flit to build a distributable package that can be installed via pip or other package managers.",flit
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),5,Command-Line Applications,Scenario,Others/Generic,Generic,"The chapter introduces two Python libraries for creating command-line applications with improved user experience. The first library, colorama, allows for the use of colors in the command-line output, which can significantly improve the user interface. The second library, begins, simplifies the creation of command-line interfaces by providing an easy-to-use interface for specifying and processing command-line options. Begins also offers additional features such as automatic handling of environment variables, config files, error handling, and logging. However, if begins seems too advanced, there are other options available such as click.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"import argparse

parser = argparse.ArgumentParser(description='Process some integers.')

parser.add_argument('integers', metavar='N', type=int, nargs='+',
                    help='an integer for the accumulator')

parser.add_argument('--sum', dest='accumulate', action='store_const',
                    const=sum, default=max,
                    help='sum the integers (default: find the max)')

args = parser.parse_args()

print(args.accumulate(args.integers))","In this example, the argparse module is used to define command-line arguments for a Python program. The code defines an argument for integers that will be accumulated, and an optional argument to specify whether to sum the integers or return the maximum integer. The `parse_args()` method is then called to parse command-line arguments and generate a Namespace object holding the values of those arguments. Finally, the program runs and prints the result of either the maximum or the sum of the given integers.","colorama, click, begins"
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),6,Graphical User Interfaces (A),Scenario,GUI,Generic,"The chapter introduces the PyQt and tkinter libraries for creating graphical user interfaces (GUIs) in Python. The chapter also highlights two lesser-known libraries: pyqtgraph and pywebview. pyqtgraph is a chart-plotting library that offers different features than the more popular matplotlib library, with a focus on interactive and real-time visualization. pywebview allows for creating a browser-like interface as a desktop client interface, which is based on tools like cefpython and Electron. The chapter provides examples and code snippets to demonstrate how to use and create GUIs with these libraries, showcasing their usefulness in developing effective and interactive desktop applications.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"from tkinter import *

class Application(Frame):
    def __init__(self, master=None):
        super().__init__(master)
        self.master = master
        self.pack()
        self.create_widgets()

    def create_widgets(self):
        self.hello_label = Label(self, text=""Hello, world!"")
        self.hello_label.pack(side=""top"")

        self.quit_button = Button(self, text=""QUIT"", fg=""red"", command=self.master.destroy)
        self.quit_button.pack(side=""bottom"")

root = Tk() 
app = Application(master=root)
app.mainloop()
","This code creates a very basic GUI using the Tkinter module in Python. It defines a class called `Application` that inherits from `Frame` and contains a constructor that sets up the application's widgets. In the constructor, two widgets are created: a label that says ""Hello, world!"" and a button that says ""QUIT"" and exits the application when clicked. Finally, an instance of the `Application` class is created with a `Tk()` instance as the master, and the `mainloop()` method is called to start the application.","pyqtgraph, pywebview, wxPython"
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),8,Web APIs with hug,Scenario,API,Generic,"The chapter introduces the hug library, which simplifies the process of building web APIs. hug allows developers to create fully-functional web APIs with minimal code and configuration, making it a valuable tool for quickly building APIs. The chapter provides examples of how to use hug to define endpoints and specify input and output formats for a web API. Hug offers automatic API documentation, which makes it easy for developers to understand and use the API. The chapter also explains how to test a hug-based API and provides examples of how to test it using different tools like curl and Postman. Overall, the hug library provides a simple and efficient way to create web APIs with minimal setup or configuration.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"import hug

@hug.get('/greet/{name}')
def greet(name):
    return f'Hello, {name}!'","This code defines a simple web service endpoint that can be accessed using the GET method on an API endpoint. In this case, sending a GET request along with a string ('name') to the endpoint /greet/{name}, where {name} is a placeholder for a variable parameter. The endpoint responds with a formatted string that greets the caller with their input. You can run this application with the `hug` command on the command line. Once the service is up and running, a user can navigate to a URL like http://localhost:8000/greet/Jane on their client, and the web service will respond to the GET request by routing it to the greet() function and returning with ""Hello, Jane!"".",hug
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),9,Dates and Times,Scenario,Others/Generic,Generic,"The chapter introduces several libraries in Python that are designed to work with dates and times. The chapter explains how to work with Python's built-in datetime module and highlights some of its more useful and powerful features, such as time zone support. The chapter also introduces two additional libraries for working with dates and times: arrow and pendulum. Arrow is a library that provides a more human-readable format for dates and times and makes it easy to do date and time arithmetic. Pendulum is another library that offers a similar interface to arrow but with additional support for things like leap years and time zones. Finally, the chapter introduces the parsedatetime library, which is designed to help with parsing natural language input into dates and times. Throughout the chapter, various examples and code snippets are used to demonstrate how to use these different libraries effectively in various tasks.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"
from datetime import datetime

# current date and time
now = datetime.now()

# print datetime object in year:month:day format
print(f'{now.year}/{now.month}/{now.day}')

# print datetime object with time
print(f'{now.hour}:{now.minute}:{now.second}')
","This code shows how to work with date and time in Python using the built-in datetime module. The `datetime.now()` function creates a datetime object representing the current date and time. We can then access the `year`, `month`, `day`, `hour`, `minute`, and `second` attributes of the object to display these values in various formats. The print statements in this code will output the current date in `year/month/day` format, and the current time in `hour:minute:second` format.","arrow, parsedatetime"
20 python libraries you aren't using but should,20 Python Libraries You Aren't Using (But Should),10,General-Purpose Libraries,Scenario,Others/Generic,Generic,"The chapter introduces two Python libraries that can improve the functionality of the Python standard library: Boltons and Cython. Boltons is a collection of small libraries that provide various functionalities such as caching, templates, and differentiation, while Cython is a library that allows developers to optimize normal Python code by converting it into C code. The chapter explains the features and functions of both libraries, and gives examples of how to use them in practical scenarios. The chapter also mentions a few other libraries such as flit, colorama, and parsedatetime, among others, that provide additional functionality to Python developers. The chapter highlights the usefulness of these libraries in improving the efficiency of Python code and suggests that adding such libraries to your program can make development much more efficient.",https://www.oreilly.com/content/20-python-libraries-you-arent-using-but-should/,"import random

# Roll a six-sided die
roll = random.randint(1, 6)

# Choose a random item from a list
items = ['apple', 'banana', 'cherry', 'date']
rand_item = random.choice(items)

# Shuffle a list
random.shuffle(items)

print(f'Roll: {roll}')
print(f'Random item: {rand_item}')
print(f'Shuffled list: {items}')","This code shows how to use the `random` module in Python to generate random numbers and randomly select elements from sequences. The `random.randint(a, b)` function generates a random integer between a and b, inclusive. The `random.choice(seq)` function chooses a random item from a sequence (`seq`), such as a list or string. The `random.shuffle(seq)` function shuffles the order of the items in a sequence. The print statements in this code show examples of using these functions to roll a six-sided die, choose a random item from a list, and shuffle the items in a list.","Cython, boltons, more-itertools, attrs, psutil, watchdog, PyContracts"
A Whirlwind Tour of Python,A Whirlwind Tour of Python,8,Built-In Data Structures,Scenario,Data and Processing,Generic,"The chapter provides an introduction to some of the core data structures in Python, including lists, tuples, sets, and dictionaries. It covers their properties, methods, and common use cases. It also briefly mentions some more specialized data structures available in the built-in collections module. The chapter concludes by recommending to refer to Python's online documentation for a complete reference of these data structures and their methods. ",https://www.oreilly.com/library/view/a-whirlwind-tour/9781492037859/,"
# Define two sets
primes = {2, 3, 5, 7}
odds = {1, 3, 5, 7, 9}

# Union of two sets
primes | odds

# Intersection of two sets
primes & odds

# Symmetric difference of two sets
primes ^ odds","This code defines two sets of integers and demonstrates three set operations: union, intersection, and symmetric difference. The `|` and `&` operators are used for union and intersection, respectively, while the `^` operator is used for symmetric difference. The output of these operations is shown in the interpreter.","list, tuple, dictionary, set, and frozenset"
A Whirlwind Tour of Python,A Whirlwind Tour of Python,15,Modules and Packages,Scenario,Others/Generic,Generic,"The chapter covers the concept of modules, which are files containing Python definitions and statements that are designed to be re-used in other Python programs. It explains how to import a module explicitly or implicitly, and how to create modules of our own. The chapter also covers the basics of creating and installing third-party package dependencies using tools such as pip. Finally, the chapter mentions the importance of using namespaces and provides a few best practices for structuring modules and packages in your Python projects.",https://www.oreilly.com/library/view/a-whirlwind-tour/9781492037859/,"# Import just the cos function and the pi constant from the math module
from math import cos, pi

# Example usage
cos(pi)","This demonstrates an example of an explicit import of module contents. Specifically, it imports the `cos` function and the `pi` constant from the `math` module using the `from` keyword and the `import` statement. After the import, the code snippet shows an example usage of the imported functions by calculating the cosine of pi.","pickle, json, csv, urllib, collections"
Beginning Game Development with Python and Pygame,Beginning Game Development with Python and Pygame: From Novice to Professional (Beginning From Novice to Professional),4,Creating Visuals,Scenario,Others/Generic,Game development,"This chapter focuses on generating visuals for computer games. It emphasizes the importance of visuals in gaming and how game developers spend a lot of time refining and improving the graphics to create an engaging gaming experience for the players. The chapter talks about using pixel power to create images and how computers store images as a grid of colors. It explains how images with an alpha channel can represent translucency and create a more professional look to the game. Additionally, it provides instructions on how to add an alpha channel to an existing image or create an image with a 3D rendering package. The chapter also covers topics like blending colors using a slider and using images in creating a game.",https://www.amazon.com/Beginning-Game-Development-Python-Pygame/dp/1590598725,"# Load image
dragon = pygame.image.load(""dragon.png"").convert_alpha()

# Draw dragon
screen.blit(dragon, (100, 100))","This code loads an image of a dragon, 'dragon.png' and converts it into a format Pygame expects. Then, the 'screen.blit' method is used to draw the dragon image to the screen at the coordinates (100, 100). This code snippet demonstrates how to load images and draw them to the screen using Pygame.","Pygame, NumPy, PyOpenGL"
Beginning Game Development with Python and Pygame,Beginning Game Development with Python and Pygame: From Novice to Professional (Beginning From Novice to Professional),12,Setting the Scene with OpenGL (A),Scenario,Others/Generic,Game development,"This chapter  introduces the basics of using OpenGL, a graphics library widely used for creating computer games. This chapter provides an overview of lighting and blending in OpenGL, and it explains how to manage lights and adjust their parameters. It also discusses how to enable and use blending to achieve effects like transparency. The chapter describes how using models and textures can help in creating a more realistic game world. It explains how to upload and render textures and talks about Texture Parameters in OpenGL. In addition, it provides a brief overview of the OBJ 3D format used for storing 3D models. Overall, this chapter covers the fundamentals of using OpenGL for creating 3D game visuals.",https://www.amazon.com/Beginning-Game-Development-Python-Pygame/dp/1590598725,"glEnable(GL_LIGHTING)
glEnable(GL_LIGHT0)
glLightfv(GL_LIGHT0, GL_AMBIENT, (0.2, 0.2, 0.2, 1.0))
glLightfv(GL_LIGHT0, GL_DIFFUSE, (0.5, 0.5, 0.5, 1.0))","This code enables lighting in OpenGL and sets up a light source. The first two lines enable lighting and specifically enable the first light source (GL_LIGHT0). Then, the next two lines set the ambient and diffuse colors of the light. The ambient color lights all the parts of a scene evenly, whereas the diffuse provides directional light to the scene. The code snippet demonstrates how to set up simple lighting in OpenGL.","NumPy, PyOpenGL"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,3,Working with Genomes,Scenario,Others/Generic,Educational purposes,"This chapter covers various recipes related to reference genomes. The chapter includes how to work with high-quality and low-quality reference genomes, traversing genome annotations, extracting genes, finding orthologues using the Ensembl REST API, and accessing gene ontology databases. The chapter also provides an introduction to computational biology, which heavily relies on reference genomes. Additionally, the chapter deals with non-model species and discusses how to analyze low-quality references for these species. ",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"Extract sequences from a genome based on their annotated
    feature. In this case, we are extracting mobile element sequences
    in the genome.

    'genome': A FASTA file containing a genomic sequence
    'resfinder': A GFF file containing annotations
    'feature': The annotated feature to be extracted
    """"""
    # Load reference
    genome = SeqIO.read(genome, format='fasta')
    # Load annotations
    annotations = SeqIO.read(resfinder, format='gff')
    # Extract features
    features = []
    for feature in annotations.features:
        if feature.type == feature:
            feature_seq = feature.extract(genome.seq)
            features.append(feature_seq)
    return features","This code snippet is for extracting a specific type of annotated feature from a genome reference sequence. In this specific example, the code is searching for mobile elements in the genome reference, but it can be modified to extract other annotations as well. The input parameters for this function include the genome reference sequence in FASTA format, the annotation file in GFF format, and the type of annotation to extract. The code uses the Biopython library and the SeqIO module to parse the genome and annotation files, respectively, and then iterates over the annotations to extract the specified feature. Finally, it returns a list of sequences for the extracted features. ","Biopython, Matplotlib, Pandas, Seaborn"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,4,Population Genetics,Scenario,Data and Processing,Educational purposes,"This chapter provides several recipes related to population genetics analysis of empirical datasets. The chapter includes how to compute several popular population genetics statistics such as Fst, perform Principal Component Analysis (PCA), and analyze population structure using Structure and Admixture. The chapter also covers how to use the Python package simuPOP for performing forward-time simulations of complex demographic scenarios under selection pressure. ",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Population structure with Admixture
from Bio.PopGen.GenePop.GenePop import count_populations
import subprocess

def run_admixture_plink(bfile, k):
    """"""
    Runs Admixture on a PLINK binary file.

    'bfile': The PLINK binary file prefix
    'k': The number of ancestries
    """"""
    # Create an initial solution using random seeds
    admixture_init = ['admixture', '--seed=123456', bfile + '.bed', str(k)]
    subprocess.call(admixture_init)
    # Estimate individual ancestries
    admixture_run = ['admixture', bfile + '.bed', str(k)]
    subprocess.call(admixture_run)
    # Read output
    f = open(bfile + '.bed.' + str(k) + '.Q', 'rb')
    output = f.read()
    f.close()
    # Process output
    output = output.strip().split('\n')
    data = []
    for ind, line in enumerate(output):
        fields = line.split(' ')
        fields = [float(x) for x in fields]
        fields = [ind + 1] + fields
        data.append(fields)
    # Done
    return data","This code snippet is for performing structure analysis of a population using Admixture. The function expects two parameters: a PLINK binary file prefix and the number of ancestral populations of interest. The initial step is to create an initial solution using random seeds which is done by calling the ""admixture"" function from the command line with the appropriate arguments. The next step is to estimate individual ancestries which is also done by calling the ""admixture"" function. The resulting output is then processed, resulting in a list of individual ancestry coefficients. The relevant packages used in this code are Bio.PopGen and subprocess.","Biopython, Matplotlib, Scipy, Seaborn"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,5,Population Genetics Simulation,Scenario,Data and Processing,Educational purposes,"This chapter provides several recipes related to population genetics simulations using Python. The chapter shows how to use simuPOP, a Python-based forward-time population genetics simulator, to simulate and analyze different selection and demographic regimes. It covers how to perform simulations to model complex demographic scenarios, simulate the coalescent with Biopython and fastsimcoal, and how to simulate population structure using stepping-stone and island models. The chapter also discusses coalescent simulations in forward and reverse time and the Wright-Fisher model. ",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Simulating Neutral Genetic Variation with Genic Selection
import simuPOP as sim
pop = sim.Population(size=[1000], ploidy=2, loci=1000)
sim.initGenotype(pop, freq=0.5)
s = 0.01    # Strength of selection
h = 0.5     # Dominance coefficient
n = 100     # Number of generations
m = 0.01    # Migration rate
mu = 1e-08  # Mutation rate
recomb_map = sim.NoReworkMap()
recomb_map.addUniformGenerator(rate=1e-08)
simuNE(pop,
       preOps=[sim.Stat(alleleFreq=range(0, 1000, 100)),
               sim.PyEval(r""'Gen = %d \n' % (gen)""),
               sim.SNPTagger(np.zeros(1000, bool), loci=['m0'])],
       matingScheme=sim.RandomMating(ops=sim.Recombinator(rates=recomb_map)),
       gen=n,
       loci=1000,
       records=sim.SNPHistory(),
       postOps=[
           sim.Stat(het=sim.HeteroZygosity(), popSize=True, Ne=True),
           sim.Stat(gt=sim.AllSimStats(mode=sim.ALL_AVAIL))
       ])","This code snippet demonstrates how to simulate genetic variation with genic selection using simuPOP – a forward-time population genetics simulation package written in Python. In this example, we simulate a population with 1000 bi-allelic loci and a population size of 1000 diploid individuals. The code sets the initial allele frequency to 0.5 for both alleles. The strength of selection is then set to 0.01, and the dominance coefficient is set to 0.5. The population is simulated over 100 generations with a migration rate of 0.01, and a mutation rate of 1e-08. A uniform recombination map is applied at a rate of 1e-08. The code then uses a number of statistics to record the results, including heterozygosity, population size, and Ne. The data is then plotted using SNPHistory.","Biopython, Numpy, Matplotlib, simuPOP"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,6,Phylogenetics,Scenario,GUI,Educational purposes,"This chapter provides several recipes related to phylogenetics analysis using Python. The chapter covers complete sequences of recently sequenced Ebola viruses to perform real phylogenetic analysis using the Biopython package. It discusses various concepts such as tree reconstruction and sequence comparisons, recursive algorithms to process tree-like structures, and visualization of phylogenetic data. The chapter also covers how to prepare data for phylogeny construction, aligning genetic and genomic data, and comparing sequences.",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Reconstructing phylogenetic trees
from Bio import AlignIO
from Bio.Phylo.TreeConstruction import (
    DistanceCalculator, DistanceTreeConstructor)
from Bio.Phylo.Consensus import (
    strict_consensus, majority_consensus, adam_consensus)
alignments = AlignIO.parse(""ebola_aligned.fasta"", ""fasta"")
aln = next(alignments)
calc = DistanceCalculator('identity')
dm = calc.get_distance(aln)
constructor = DistanceTreeConstructor()
tree = constructor.nj(dm)","This code snippet shows how to reconstruct phylogenetic trees using the BioPython package. The code imports AlignIO to parse multiple sequence alignments in FASTA format. It then reads an alignment file, ""ebola_aligned.fasta,"" and selects the first alignment in the iterator by calling next. The code uses a Distance Calculator to calculate the pairwise distances between each species in the selected alignment. The DistanceTreeConstructor then constructs a tree using the Neighbour Joining method. The resulting tree is represented in a Newick format and printed to the console. ","DendroPy, Matplotlib, NetworkX, Seaborn"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,7,Using the Protein Data Bank,Scenario,Others/Generic,Educational purposes,"This chapter explains how to retrieve and process Protein Data Bank (PDB) files using Biopython. The chapter discusses various tasks such as extracting more information from a PDB file, computing molecular distances on a PDB file, performing geometric operations, implementing a basic PDB parser, and animating with PyMol. The chapter also covers parsing mmCIF files using Biopython.",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Extracting more information from a PDB file
from Bio import PDB
parser = PDB.PDBParser()
structure = parser.get_structure('BGA', '1BGA.pdb')
print(structure.header['classification'])
print(structure.header['deposition_date'])","This code snippet shows how to extract more information from a PDB file using Biopython's PDB module. The code reads a PDB file, ""1BGA.pdb,"" using the PDBParser(). It then accesses the classification and deposition date information from the PDB file header using structure.header. Finally, the header information is printed to the console.","Biopython, Matplotlib, PyMOL"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,8,Other Topics in Bioinformatics,Scenario,Others/Generic,Educational purposes,"This chapter covers topics such as processing and analyzing data made available by the Global Biodiversity Information Facility (GBIF), visualizing complex networks using Cytoscape, working with geo-referenced data, and map-based services. The chapter utilizes several Python libraries such as GeoPy, Shapely, and Basemap. The chapter also covers how to store and retrieve data from SQL and NoSQL databases using Python. Finally, the chapter discusses the use of cloud computing and batch processing in bioinformatics.",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Using the GeoPy library to retrieve location data
from geopy.geocoders import Nominatim
geolocator = Nominatim(user_agent=""geoapiExercises"")
location = geolocator.geocode(""175 5th Avenue NYC"")
print((location.latitude, location.longitude))","This code snippet shows how to use the GeoPy library to retrieve location data based on a given address. The code first imports the Nominatim class from the GeoPy package. It then initializes a geolocator object with the user_agent parameter. The code retrieves the address ""175 5th Avenue NYC"" using the geolocator.geocode() method, and stores the resulting location information in the location variable. Finally, the code prints the latitude and longitude of the location to the console.","Biopython, Cytoscape, Geopy, Matplotlib, Pandas"
Bioinformatics with Python Cookbook,Bioinformatics with Python Cookbook,9,Python for Big Genomics Datasets,Scenario,API,Educational purposes,"This chapter discusses how to handle big genomic datasets by leveraging modern hardware technologies. The chapter covers topics such as parallel processing using multiprocessing, implementing parallelism using IPython Parallel, and using Dask to perform parallel processing in a distributed computing environment. The chapter also discusses cluster usage and various code optimization platforms such as Numba and Cython. Finally, the chapter explores techniques for memory optimization and the effective use of memory-mapped files.",https://www.amazon.co.jp/-/en/Tiago-Antao/dp/1782175113,"# Using Dask to parallelize a read count
import dask.bag as db
from Bio import SeqIO
import glob
filenames = glob.glob('*.fasta')
b = db.from_sequence(filenames)
records = b.map(lambda x: SeqIO.to_dict(SeqIO.parse(x, 'fasta')))
single_dict = records.map(lambda d: {k: str(v.seq) for k, v in d.items()}).\
        fold(lambda a, b: dict(list(a.items()) + list(b.items())), 
             initial=dict())
print(len(single_dict))","This code snippet shows how to use the Dask library to parallelize a read count on multiple genomic sequence files. The code first imports the necessary modules such as dask.bag and SeqIO from the Biopython package. It then reads in multiple sequence files with the same extension using the glob module. The code creates a Dask bag from the filenames, and maps the SeqIO.to_dict() method over each file. The code uses lambda functions to transform the sequence dictionaries into a single dictionary, and counts the number of sequences in the dictionary by printing its length to the console.","Biopython, Numba, Scikit-learn"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,3,Data Meant to Be Read by Machines,Scenario,API,Data analysis,"This chapter discusses different file formats for data storage. The chapter distinguishes between human-readable formats, such as Microsoft Word documents, and machine-readable formats that computers understand easily. The focus of the chapter is the latter category of formats, including Comma-Separated Values (CSV), JavaScript Object Notation (JSON), and Extensible Markup Language (XML). The chapter also explains how to read and use these data formats with Python scripts to communicate findings to end-users.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"from xml.etree import ElementTree as ET tree = ET.parse('data-text.xml')
    root = tree.getroot()
    data = root.find('Data')
    all_data = []
for observation in data: record = {}
for item in observation:
            lookup_key = item.attrib.keys()[0]
if lookup_key == 'Numeric':
rec_key = 'NUMERIC'
rec_value = item.attrib['Numeric']
else:
rec_key = item.attrib[lookup_key] rec_value = item.attrib['Code']
            record[rec_key] = rec_value
        all_data.append(record)
print all_data","This code snippet is designed to parse an XML file named 'data-text.xml' using ElementTree. It loads the XML, accesses its root, and then specifically targets a child element named 'Data'. The code iterates through each child element ('observation') within 'Data', extracting key-value pairs based on the attributes of these child elements. For each observation, it creates a dictionary where keys are determined by the attribute's name (with special handling for the 'Numeric' attribute) and values are extracted from the attributes. These dictionaries are then aggregated into a list, all_data, representing the structured data extracted from the XML. Finally, the list all_data is printed, showcasing the organized data from the XML file.","csv, json, xml"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,4,Working with Excel Files (B),Scenario,Others/Generic,Data analysis,"In this chapter, the focus is on how to extract information from Excel files using Python packages such as xlrd, pandas, and openpyxl. The chapter explains that extracting data from Excel files can be challenging because Excel files can contain multiple sheets, formats, and formulas that might not be visible. The author explains the process of installing these external libraries and then demonstrates how to open a specified Excel file, select a particular sheet, and extract required data using Python packages. Finally, the author combines all the steps to create a script that can parse Excel files and extract information from them.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"# Import the required library
import pandas as pd

# Assign an Excel file to a variable
file = 'example.xlsx'

# Load the first sheet of the Excel file into a data frame
df = pd.read_excel(file)","This code snippet demonstrates how to read an Excel file and load its data into a data frame using the Python package - Pandas. It imports the package using the statement: `import pandas as pd`. Then, assigns the 'example.xlsx' file to a variable `file`. Finally, the code loads the first sheet of the Excel file to a data frame using the `read_excel()` function in Pandas and saves the data frame to a variable `df`. This snippet provides a simple example of how to read Excel files using Pandas, and similar functions in other packages like xlrd and openpyxl work similarly.","xlrd, openpyxl"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,5,PDFs and Problem Solving in Python,Scenario,Others/Generic,Data analysis,"The chapter explains how to extract useful information from PDF files using Python. It starts by highlighting some of the challenges associated with using PDFs as a data source, including their complexity and variability. The chapter then discusses some available Python packages that make it possible to work with PDFs efficiently, such as pdfminer, slate, and PyPDF2. The author walks through various examples of analyzing PDFs with Python, including reading text from a PDF, identifying tables, and extracting data from tables. The author also provides some general strategies for problem-solving when working with PDFs. Finally, the chapter concludes with an exercise where the reader can practice using Python to extract data from PDF files.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"# Import Required Library 
import slate3k as slate

# Open and Read a PDF File 
with open('example.pdf', 'rb') as f:
    document_text = slate.PDF(f)
print(document_text)","This code snippet demonstrates how to import the slate3k library and use it to read a PDF file and extract its text. First, the required library 'slate3k' is imported using the statement: `import slate3k as slate`. Then, a PDF file named 'example.pdf' is opened in the read-binary mode ('rb') using the 'open' function. ","PyPDF2, pdfminer"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,8,Data Cleanup: Standardizing and Scripting,Scenario,Others/Generic,Data analysis,"The chapter discusses how to standardize and automate the data cleanup process in Python. It starts by explaining how to normalize and standardize data using techniques such as scaling data, creating dummy variables, and encoding categorical variables. The chapter also provides an overview of the basic principles of scripting data cleanup, such as using automation tools and techniques to increase efficiency. Additionally, it introduces readers to the concept of scripting data cleanup, teaching how to write a cleanup script for specific data transformations. Finally, the chapter concludes by demonstrating how to test the cleanup script using new data. In summary, the chapter provides a comprehensive guide to standardizing the data cleanup process in Python, using automation tools and techniques to make the process more efficient and less prone to errors.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"import dataset
db = dataset.connect('sqlite:///data_wrangling.db') table = db['unicef_survey']
for row_num, data in enumerate(zipped_data): for question, answer in data:
            data_dict = {
                'question': question[1],
                'question_code': question[0],
                'answer': answer,
                'response_number': row_num,
                'survey': 'mn',
            }
        table.insert(data_dict)","This code demonstrates the process of importing data into a SQLite database using the dataset library. It starts by importing the dataset module and establishing a connection to a SQLite database named 'data_wrangling.db'. A table named 'unicef_survey' is then accessed or created in this database. The code iterates over 'zipped_data', which appears to be a collection of tuples pairing survey questions and answers. For each row (enumerated by 'row_num') and each question-answer pair in the data, it creates a dictionary 'data_dict'. This dictionary includes the question text, its code, the corresponding answer, the response number (which serves as a unique identifier for each survey response), and a hard-coded survey identifier ('mn'). Finally, each of these dictionaries is inserted as a new record into the 'unicef_survey' table in the database, effectively storing each survey response as a separate row in the table.","pandas, NumPy, re"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,9,Data Exploration and Analysis,Scenario,Others/Generic,Data analysis,"This chapter provides an overview of how to explore and analyze your data programmatically by using Python's libraries, which are specifically designed for data manipulation and analysis. The chapter covers various topics, including but not limited to, exploring your data, importing data, exploring table functions, joining numerous datasets, identifying correlations, identifying outliers, creating grouping, separating and focusing your data, drawing conclusions, and documenting your conclusions. This chapter also focuses on the importance of presenting your data accurately and avoiding storytelling pitfalls, visualizing your data, presentation tools, publishing your data, and using available sites. By using Python's libraries, this chapter emphasizes how you can analyze data more effectively and efficiently.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"africa_cpi_cl = cpi_and_cl.where(lambda x: x['continent'] == 'africa')
for r in africa_cpi_cl.order_by('Total (%)', reverse=True).rows:
print ""{}: {}% - {}"".format(r['Country / Territory'], r['Total (%)'],
                                    r['CPI 2013 Score'])
import numpy
print numpy.corrcoef(
[float(t) for t in africa_cpi_cl.columns['Total (%)'].values()],
[float(c) for c in africa_cpi_cl.columns['CPI 2013 Score'].values()])[0, 1]
    africa_cpi_cl = africa_cpi_cl.compute([('Africa Child Labor Rank',
                                            agate.Rank('Total (%)', reverse=True)),
])
    africa_cpi_cl = africa_cpi_cl.compute([('Africa CPI Rank',
                                            agate.Rank('CPI 2013 Score')),
])","
This Python code snippet filters a dataset to focus on African countries, sorting and printing each country's name alongside its child labor percentage and Corruption Perceptions Index (CPI) score for 2013. It then calculates the correlation coefficient between child labor percentage and CPI scores using NumPy, indicating a statistical relationship between these two factors. Finally, the script ranks African countries based on both child labor percentage and CPI score, adding these rankings as new columns in the dataset, thus providing a comparative view of the prevalence of child labor and perceived corruption across African nations.","pandas, NumPy, matplotlib, seaborn"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,10,Presenting Your Data,Scenario,Others/Generic,Data analysis,"This chapter discusses the importance of telling a story with data, knowing your audience, and avoiding storytelling pitfalls. It also covers various methods of visualizing data such as charts, maps, and interactives as well as presenting data using words, images, video, and illustrations. The chapter then delves into presentation tools, publishing data, and using various available sites and open-source platforms to present data. In summary, this chapter provides practical guidance on how to present data effectively and communicate your findings to your intended audience.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"import matplotlib.pyplot as plt plt.plot(africa_cpi_cl.columns['CPI 2013 Score'],
               africa_cpi_cl.columns['Total (%)'])
    plt.xlabel('CPI Score - 2013')
    plt.ylabel('Child Labor Percentage')
    plt.title('CPI & Child Labor Correlation')
plt.show()","This Python code snippet uses Matplotlib, a plotting library, to create a scatter plot visualizing the relationship between the Corruption Perceptions Index (CPI) scores for 2013 and the Child Labor percentages in African countries. It accesses the CPI scores and Child Labor percentages from the 'africa_cpi_cl' dataset, plotting the CPI scores on the x-axis and the Child Labor percentages on the y-axis. Labels for the x-axis ('CPI Score - 2013') and y-axis ('Child Labor Percentage') are set, along with a title for the plot ('CPI & Child Labor Correlation'). Finally, plt.show() is called to display the plot, which is expected to provide a visual representation of the correlation (if any) between the two variables in the context of African countries.","matplotlib, Pillow"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,11,Web Scraping: Acquiring and Storing Data form the Web,Scenario,Others/Generic,Data analysis,"This chapter talks about web scraping, which is the process of acquiring and storing data from websites. The chapter provides information on how to analyze a web page, inspect its markup structure, analyze how the page loads, and interact with JavaScript if needed. It also covers how to request web pages from the internet and how to store the data in different formats. Furthermore, the chapter provides information on web scraping tools such as Beautiful Soup, Scrapy, and Selenium. In summary, this chapter discusses the basis of web scraping, how to perform web scraping, and how to store the data acquired effectively.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"import requests
from bs4 import BeautifulSoup

url = 'http://example.com'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')
articles = soup.find_all('article')

for article in articles:
    title = article.h2.a.text.strip()
    headline = article.h3.text.strip()
    summary = article.find('div', class_='summary').text.strip()

    print('Title:', title)
    print('Headline:', headline)
    print('Summary:', summary)
    print('-----------------------')","This code sends a GET request to a URL, and then uses Beautiful Soup to parse the HTML content of the response. It finds all the articles on the page and then extracts the title, headline, and summary from each, before printing the information to the console. Note that the `class_` argument is used instead of `class` to avoid a syntax error, as `class` is a reserved word in Python.","BeautifulSoup, urllib, requests"
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,12,Advance Web Scraping: Screen Scrapers and Spiders,Scenario,Others/Generic,Data analysis,"This chapter covers more complex web scraping concepts, including browser-based parsing, screen-reading with Selenium and Ghost.Py, spidering the web, and building spiders with Scrapy. It provides information on how to use different web scraping tools and techniques to automate the data collection process and scrape large volumes of data. The chapter also provides practical examples such as how to crawl whole websites with Scrapy, and how to use network proxies and parallel processing for faster and more efficient web scraping. In summary, this chapter provides an in-depth exploration of advanced web scraping techniques that can help automate data scraping processes for large datasets.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"import scrapy

class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)","This script crawls the first page of quotes.toscrape.com and extracts the text, author, and tags for each quote on the page using Scrapy's CSS selector. It then follows the link to the next page and repeats the process until there are no more pages left to crawl. Finally, it yields a Python dictionary containing the scraped data. This is just one example of what's possible with Scrapy. Scrapy can be customized to handle more complex websites and can also be used in combination with other Python libraries to scrape data from various online sources.",Scrapy
Data Wrangling with Python,Data Wrangling with Python: Tips and Tools to Make Your Life Easier,13,APIs,Scenario,API,Data analysis,"This chapter covers how to use APIs to programmatically access and retrieve data from different web services. It provides information on API features such as the differences between REST (Representational State Transfer) and streaming APIs, rate limits, tiered data volumes, and API keys and tokens. It also provides practical examples of how to create a Twitter API key and use the Tweepy library to retrieve data from Twitter's REST and streaming APIs. The chapter also provides practical advice for working with APIs, such as how to interpret API error messages and how to handle API rate limits. In summary, this chapter covers different types of APIs available, their features, and how to work with them effectively to retrieve data programmatically.",https://www.oreilly.com/library/view/data-wrangling-with/9781491948804/,"import tweepy

# Authenticate to Twitter API
auth = tweepy.OAuthHandler(""consumer_key"", ""consumer_secret"")
auth.set_access_token(""access_token"", ""access_token_secret"")
api = tweepy.API(auth)

# Search for Tweets
query = ""Python""
tweets = api.search(q=query, lang=""en"", count=10)

# Print Tweets
for tweet in tweets:
    print(""-"" * 50)
    print(f""User: {tweet.user.name}"")
    print(f""Screen Name: @{tweet.user.screen_name}"")
    print(f""Location: {tweet.user.location}"")
    print(f""Tweet: {tweet.text}"")","This code snippet combines Tweepy with the Twitter API to search for 10 tweets containing the word ""Python"" that are written in English language. The API request is made through Tweepy's `search` method. The authentication details of the Twitter App must be provided at the beginning for that. Then, it prints some information for each of the tweets collected. There are other functionalities provided by the Tweepy library which can be found in their documentation. This code is just an example of how to get started with Tweepy library to access Twitter API through Python.","requests, tweepy"
Dive Into Python - Mark Pilgrim,Dive into Python,8,HTML Processing,Scenario,Others/Generic,Generic,"In this chapter, the author has discussed HTML processing and how to extract data from HTML documents using sgmllib module. The chapter also explains BaseHTMLProcessor and dialect.py. The author has explained the handling of command-line arguments and has covered various other topics like locals and globals, Dictionary-based string formatting, and quoting attribute values. Finally, the chapter concludes with putting all the concepts together and a summary of the chapter.",https://diveintopython3.net,"from sgmllib import SGMLParser

class MyParser(SGMLParser):
    def __init__(self):
        SGMLParser.__init__(self)
        self.links = []
    def start_a(self, attrs):
        href = [v for k, v in attrs if k=='href']
        if href:
            self.links.extend(href)

html = """"""
<html>
<head><title>Test</title></head>
<body>
<a href=""http://www.google.com"">A link to Google</a>
</body>
</html>
""""""
parser = MyParser()
parser.feed(html)
print(parser.links)  # prints ['http://www.google.com']","This code defines a class `MyParser` that inherits from `SGMLParser` and overrides the `start_a` method, which is called whenever the parser encounters an `<a>` tag. In this method, the href attribute is extracted and added to a list of links. The code then creates an instance of `MyParser`, feeds it an HTML document, and prints the list of links that were extracted.","sgmllib.py, BaseHTMLProcessor.py, dialect.py."
Dive Into Python - Mark Pilgrim,Dive into Python,9,XML Processing,Scenario,Others/Generic,Generic,"In this chapter, the author explains the basics of XML processing and has given an overview of XML Parser and Libraries like cElementTree, ElementTree, and minidom. The author has covered topics like parsing XML, searching for elements, and accessing element attributes while explaining their usage with code samples. The chapter also includes a discussion on Unicode and concludes with a summary of the chapter.",https://diveintopython3.net,"import xml.etree.ElementTree as ET

# create an XML string
xml_string = '''
<catalog>
    <book id=""bk101"">
        <author>Gambardella, Matthew</author>
        <title>XML Developer's Guide</title>
        <genre>Computer</genre>
    </book>
</catalog>
'''

# create an ElementTree object from the XML string
root = ET.fromstring(xml_string)

# access elements and attributes using ElementTree API
for book in root.findall('book'):
    id = book.get('id')
    author = book.find('author').text
    title = book.find('title').text
    genre = book.find('genre').text

    print('{} ({}): {}'.format(title, genre, author))","This code demonstrates how to create an ElementTree object from an XML string, access elements and attributes using ElementTree API, and print the results. It defines an XML string with a `<catalog>` element that contains a `<book>` element with attributes and child elements. It then creates an ElementTree object from the XML string, iterates over all `<book>` elements, and extracts their attributes and child elements using the `find()` and `get()` methods of ElementTree. Finally, it prints the extracted attributes and child elements in a formatted string.","xml.dom.minidom, xml.dom.ext, xml.xpath."
Dive Into Python - Mark Pilgrim,Dive into Python,11,HTTP Web Services,Scenario,API,Generic,"In this chapter, the author explains the basics of HTTP and demonstrates how to fetch data over HTTP using urllib2 library. The author has covered topics like HTTP request methods (GET, POST, etc.), User-agent, redirects, last-modified/if-modified-since, ETag/if-none-match, and compression. The chapter also includes a discussion on debugging HTTP web services and how to handle errors. Finally, the author provides a code sample that demonstrates different HTTP features.",https://diveintopython3.net,"import urllib2

# set the URL and User-Agent header
url = 'http://www.example.com/'
user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'
headers = {'User-Agent': user_agent}

# create a request object with URL and headers
request = urllib2.Request(url, headers=headers)

# fetch the URL and print the response body
response = urllib2.urlopen(request)
print(response.read())","This code imports the `urllib2` module and sets the URL and User-Agent header for a web page. It then creates a `urllib2.Request` object with the URL and headers and sends a request to fetch the URL using `urllib2.urlopen`. Finally, it prints the response body of the web page. Note that the User-Agent header is set to a Mozilla browser agent to mimic a typical web browser request.","httplib, urllib, urlparse, StringIO, gzip."
Dive Into Python - Mark Pilgrim,Dive into Python,12,SOAP Web Services,Scenario,Others/Generic,Generic,"In this chapter, the author explains the basics of SOAP (Simple Object Access Protocol) and shows how to communicate with SOAP web services using SOAPpy library. The author has covered topics like installing SOAP libraries, creating a SOAP client, invoking SOAP methods, and handling SOAP faults. The chapter also includes a discussion on WSDL (Web Services Description Language) and how to use it to discover SOAP web services. Finally, the author provides a code sample that demonstrates how to use SOAPpy to communicate with a SOAP web service.",https://diveintopython3.net,"import SOAPpy

# create a SOAP client
server_url = 'http://localhost:8080/soap'
namespace = 'urn:ExampleService'
client = SOAPpy.SOAPProxy(server_url, namespace)

# invoke a SOAP method
result = client.calculate(a=5, b=10)

# print the result
print(result)","This code imports the `SOAPpy` module and creates a SOAP client that points to a SOAP web service at `http://localhost:8080/soap`. It then invokes a SOAP method called `calculate` with parameters `a=5` and `b=10` using the client `SOAPProxy` object. Finally, it prints the result of the SOAP method call. Note that the `namespace` parameter is set to the namespace of the SOAP web service, which is required for method invocation. The `SOAPpy` module also supports other SOAP protocols such as WSDL, which can be used to discover and access SOAP web services.","PyXML, fpconst, SOAPpy."